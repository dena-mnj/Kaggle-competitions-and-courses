# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load
​
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
​
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
​
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
​
# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# Load the files
train_data = pd.read_csv("/kaggle/input/titanic/train.csv")
test_data = pd.read_csv("/kaggle/input/titanic/test.csv")

# Ground truth was only provided for the train dataset 


# Missing Data
print('Number of missing data for traning set')
print(train_data.isnull().sum())

print('Number of missing data for test set')
print(test_data.isnull().sum())

# Embarked has two missing values in training set and therefore these rows can be dropped 
train_data = train_data[train_data['Embarked'].notna()]

# Fare has one missing value in test set. This row was dropped
test_data = test_data[test_data['Fare'].notna()]

# Age has many missing values. Nan was replaced by the median in both train and test dataset 
train_data['Age'].fillna(train_data['Age'].median(), inplace=True)
test_data['Age'].fillna(test_data['Age'].median(), inplace=True)

# Majority of values in Cabin column are missing in both datasets. Cabin column was excluded.
train_data.drop(['Cabin'], axis = 1, inplace = True)
test_data.drop(['Cabin'], axis = 1, inplace = True)

print('Number of missing data for traning set')
print(train_data.isnull().sum())

print('Number of missing data for test set')
print(test_data.isnull().sum())

# Feature engineering 
# A new column called family size was created

train_data['FamilySize']=train_data['SibSp']+train_data['Parch']+1
test_data['FamilySize']=test_data['SibSp']+test_data['Parch']+1

train_data = pd.get_dummies(train_data,columns = ["Sex","Embarked"],
                             prefix=["Sex","Embarked"])
test_data = pd.get_dummies(test_data,columns = ["Sex","Embarked"],
                             prefix=["Sex","Embarked"])
test_data.head()


select = ['Pclass','Age','Fare','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S','FamilySize','Survived']
train_data=train_data[select]

# Heatmap and correlation plot
corr = train_data.corr()
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict #prediction

features = ['Pclass','Age','Fare','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S','FamilySize']

y = train_data['Survived']
x = train_data[features]
x_train, x_test, y_train, y_test = train_test_split(x, y,train_size=0.8,random_state=42)

# Random Forest Model
model = RandomForestClassifier(random_state=0)
model.fit(x_train,y_train)
rf_pred = model.predict(x_test)

# Accuracy
print('Random Forest model accuracy:')
print('Accuracy score: {:.2f}'
     .format(accuracy_score(rf_pred, y_test)))

cv_rf = cross_val_score(model, x, y)
y_pred = cross_val_predict(model,x,y,cv=10)

print('Cross Validation Score:',round(cv_rf.mean(),4))
print('Confusion Matrix:')
# Confusion Matrix
sns.heatmap(confusion_matrix(y, y_pred,normalize='true'),
            annot=True,
            cmap=sns.diverging_palette(20, 220, n=200))
            
            
# Logistic Regression 
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression().fit(x_train, y_train)
lr_pred = lr.predict(x_test)

# Accuracy
print('Lostic Regression model accuracy:')
print('Accuracy score: {:.2f}'
     .format(accuracy_score(lr_pred, y_test)))

cv_lr = cross_val_score(lr, x, y)
y_pred = cross_val_predict(lr,x,y,cv=10)

print('Cross Validation Score:',round(cv_rf.mean(),4))
print('Confusion Matrix:')
# Confusion Matrix
sns.heatmap(confusion_matrix(y, y_pred,normalize='true'),
            annot=True,
            cmap=sns.diverging_palette(20, 220, n=200))
            
    
    
# KNN Classifier 
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier().fit(x_train, y_train)
knn_pred = knn.predict(x_test)

# Accuracy
print('KNN model accuracy:')
print('Accuracy score: {:.2f}'
     .format(accuracy_score(knn_pred, y_test)))

cv_knn = cross_val_score(knn, x, y)
y_pred = cross_val_predict(knn,x,y,cv=10)

print('Cross Validation Score:',round(cv_knn.mean(),4))
print('Confusion Matrix:')
# Confusion Matrix
sns.heatmap(confusion_matrix(y, y_pred,normalize='true'),
            annot=True,
            cmap=sns.diverging_palette(20, 220, n=200))
            
            
# Support Vector Machine  
from sklearn.svm import SVC

clf = SVC().fit(x_train, y_train)
svm_pred = knn.predict(x_test)

# Accuracy
print('Support Vector Machine model accuracy:')
print('Accuracy score: {:.2f}'
     .format(accuracy_score(svm_pred, y_test)))

cv_svm = cross_val_score(clf, x, y)
y_pred = cross_val_predict(clf,x,y,cv=10)

print('Cross Validation Score:',round(cv_svm.mean(),4))
print('Confusion Matrix:')

# Confusion Matrix
sns.heatmap(confusion_matrix(y, y_pred,normalize='true'),
            annot=True,
            cmap=sns.diverging_palette(20, 220, n=200))
            
# Model Comparison 
eva=pd.DataFrame({
                  'Model':['Random Forest','Logistic Regression','KNN','Support Vector Machine'],
                  'Score':[cv_rf.mean(),cv_lr.mean(),cv_knn.mean(),cv_svm.mean()]
})
print(eva)


# Optimize Models

from sklearn.model_selection import GridSearchCV

model= SVC()
grid_values = {'kernel': ['rbf','linear'], 
                  'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100],
                  'C': [1, 10, 50, 100,150,200,300, 1000]}


model_svm = GridSearchCV(model, param_grid = grid_values,cv=10,verbose=1).fit(x_train, y_train)

print(model_svm.best_estimator_)

# Best score
print(model_svm.best_score_)

model= RandomForestClassifier()

grid_values = {"n_estimators":range(100,1000,100)}

model_rf = GridSearchCV(model, param_grid = grid_values,cv=10,verbose=1).fit(x_train, y_train)

print(model_rf.best_estimator_)

# Best score
print(model_rf.best_score_)


print(model_rf.best_score_)
print(model_rf.best_estimator_)


# knn classifier
model= KNeighborsClassifier()
grid_values = {'n_neighbors':[4,5,6,7],
              'leaf_size':[1,3,5],
              'algorithm':['auto', 'kd_tree'],
              'n_jobs':[-1]}


model_knn = GridSearchCV(model, param_grid = grid_values,verbose=1).fit(x_train, y_train)

print(model_knn.best_estimator_)

# Best score
print(model_knn.best_score_)

# Loggistic Regression
from sklearn.model_selection import RepeatedStratifiedKFold
model= LogisticRegression()
grid_values = {'solver' = ['newton-cg', 'lbfgs', 'liblinear'],
               'penalty' = ['none', 'l1', 'l2', 'elasticnet'],
               'C' = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]}

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

model_lr= GridSearchCV(model, param_grid = grid_values,cv=cv,verbose=1).fit(x_train, y_train)


print(model_lr.best_estimator_)

# Best score
print(model_lr.best_score_)


print(test_data.head())
print(features)

x_test = test_data[features]

model = RandomForestClassifier(n_estimators=600,random_state=0).fit(x_train,y_train)
model.fit(x_train,y_train)
rf_pred = model.predict(x_test)


output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': rf_pred})
output.to_csv('my_submission.csv', index=False)
print("Your submission was successfully saved!")
